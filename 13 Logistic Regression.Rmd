---
title: "Logistic Regression"
subtitle: "MCB Consulting Group (MKTG 411)"
output: 
  ioslides_presentation:
    widescreen: true
    css: style.css
---

## Marketing Analytics Process

<center>
![](Figures/process_model.png){width=900px}
</center>

---

![](Figures/hex_tidymodels.png){width=500px}

## Inferential Modeling Workflow

<center>
![](Figures/workflow-inference.png){width=900px}
</center>

## Consider | Consider the Story

Remember, your story about how explanatory variables affect your outcome of interest is the beginning of a model.

Thus far, we've been building linear regression models for Kroger to evaluate drivers of sales in their soup category. Now, they've come to us with a new request. We are to identify drivers of what management describes as "top-selling" sales periods - sales periods where category sales are significantly higher than usual.

What might drive a "top-selling" sales period? What is fundamentally different about our independent variable in this model vs. our previous models?

## Build | Build a Statistical Model

We translate our story into math and build a statistical model. So far, we've used:

$$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p + \epsilon, \text{ where } \epsilon \sim Normal(0, 1)$$

- $y$ is the *outcome* variable.
- $x_1$ through $x_p$ are the *explanatory* variables (a.k.a., predictors).
- $\beta_0$ is the *intercept* parameter.
- $\beta_1$ through $\beta_p$ are the *slope* parameters.
- $\epsilon$ is the *error* term.

But what if $y$ is **discrete** rather than continuous?

---

Instead of a *linear* regression where the outcome is continuous, we need a model for **classification** where the outcome is discrete (specifically binary, where $y$ is zero or one).

Instead of modeling $y$, what if we modeled the probability of $y$?

$$\text{Pr}(y) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p$$

Closer, but still not right.

$$\text{Pr}(y) \in (0, 1)$$
$$\beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p \in (âˆ’\infty, \infty)$$

We need to transform the linear model so that it equals a probability.

## Logistic Regression

This resulting classification model is called **logistic regression** (named after the function applied to the linear model).

$$
\text{Pr}(y) = {\exp\left(\beta_0 + \beta_1 x_1 + \dotsc + \beta_px_p\right) \over 1 + \exp\left(\beta_0 + \beta_1 x_1 + \dotsc + \beta_px_p\right)}
$$

- $y$ is a *binary* outcome variable.
- $x_1$ through $x_p$ are the *explanatory* variables (a.k.a., predictors).
- $\beta_0$ is the *intercept* parameter.
- $\beta_1$ through $\beta_p$ are the *slope* parameters.

---

A few things to note:

- The error term is part of the probability rather than added on.
- We have the same goal as before: estimate the parameters.
- Logistic regression is also known as **binomial regression** or a **logit model**.
- There are many models that follow this process of transforming a linear model to work with different outcome variables. Altogether they are known as **generalized linear models** (GLMs).
- Logistic regression is the most commonly used GLM.

## Let's Implement a Logistic Regression...

```{r include=FALSE}
# Load packages.
library(tidyverse)
library(tidymodels)
# library(ggplot2)
```

## Preprocess | Preprocess Data for Modeling

Before implementing our model, we need to *preprocess the data* to make it appropriate for the model.

```{r}
soup_data <- read_csv("soup_data.csv")
```

## Defining our Outcome Variable

Part of the pre-processing in this case is creating our outcome variable.

```{r echo = FALSE}
soup_data |>
  ggplot(aes(x = Sales)) +
  geom_histogram()
```

---

If we look at the 80th percentile, it is about $20,000.

```{r}
soup_data |>
  summarize(p95_sales = quantile(Sales, 0.80))
```

## Define Top Selling

```{r}
# Import soup data.
soup_data <- soup_data |> 
  mutate(top_selling = if_else(Sales > 20000, 1, 0) |> factor())
```

---

We'll also split the data into training and testing sets.

```{r}
# Split data.
soup_split <- initial_time_split(soup_data, prop = 0.90)
```

## Your Turn!

Define a new recipe called `soup_recipe` based on the training data that prepares data for a model where `top_selling` is our dependent variable and `Any_Price_Decr_Spend` and `Retailer_Trade_Areas` are our independent variables. Include appropriate pre-processing steps and prep the recipe.

## Expected Output

```{r echo = FALSE}
# Prepare a preprocessing recipe.
(soup_recipe <- training(soup_split) |> 
  recipe(top_selling ~ Any_Price_Decr_Spend + Retailer_Trade_Areas) |> 
  step_log(all_numeric(), offset = 1) |> 
  step_dummy(Retailer_Trade_Areas) |> 
  prep())
```

## Solution

```{r eval = FALSE}
# Prepare a preprocessing recipe.
(soup_recipe <- training(soup_split) |> 
  recipe(top_selling ~ Any_Price_Decr_Spend + Retailer_Trade_Areas) |> 
  step_log(all_numeric(), offset = 1) |> 
  step_dummy(Retailer_Trade_Areas) |> 
  prep())
```

# Apply the recipe to the training and testing data.

```{r}
soup_training <- soup_recipe |>
  bake(training(soup_split))

soup_testing <- soup_recipe |>
  bake(testing(soup_split))
```

## Fit | Specify the Model Type and Engine and Fit

Remember, we specify the model and engine and fit the model. When we *fit* the model (a.k.a., training, calibrating, or estimating the model) we are getting parameter estimates.

```{r}
# Fit a logistic regression.
fit <- logistic_reg() |> 
  set_engine("glm") |> 
  fit(top_selling ~ ., data = soup_training)
```

## Evaluate | Parameter Estimates, Significance, and Predictive Fit

Remember, our goal is to use the model to *estimate* the unobserved parameters from the data. In other words, parameter estimates are the information we are extracting from the data to inform our managerial decision.

```{r eval=FALSE}
# Compare parameter estimates.
tidy(fit, conf.int = TRUE) |> 
  ggplot(aes(y = term)) + 
  geom_point(aes(x = estimate)) + 
  geom_errorbar(aes(xmin = conf.low, xmax = conf.high)) +
  geom_vline(xintercept = 0, color = "red")
```

---

```{r echo=FALSE}
# Compare parameter estimates.
tidy(fit, conf.int = TRUE) |> 
  ggplot(aes(y = term)) + 
  geom_point(aes(x = estimate)) + 
  geom_errorbar(aes(xmin = conf.low, xmax = conf.high)) +
  geom_vline(xintercept = 0, color = "red")
```

---

```{r}
# What about interpreting the parameter estimates?
tidy(fit, conf.int = TRUE)
```

Each $\beta$ represents the expected change in the **log-odds** of "success" (getting a one in the binary outcome) for a one-unit increase in the value of X, holding all other variables fixed (relative to the baseline level if discrete).

---

$$
\text{Pr}(\text{top_selling}) = {\exp\left(-6.74 + 0.75 \times \text{Any_Price_Decr_Spend} + \ldots \right) \over 1 + \exp\left(-6.74 + 0.75 \times \text{Any_Price_Decr_Spend} + \ldots \right)}
$$

Use $plogis(\beta)$ to obtain the expected change in *probability* of success relative to a baseline level of 0 for all other predictors (including the intercept!)

---

```{r}
# Interpreting the parameter estimates as probabilities.
tidy(fit, conf.int = TRUE) |> 
  mutate(
    estimate = plogis(estimate),
    conf.low = plogis(conf.low),
    conf.high = plogis(conf.high)
  )
```


## Why is the probability interpretation so weird?

Unfortunately, we don't have an 'easy' interpretation of the coefficients' effects on the probability of success. Why?

<center>

![](Figures/logistic_function.png){width=600px}

<center>

---

Remember that instead of measuring how well a model fits the *data*, we should measure how well a model *predicts*. This predictive fit doesn't have the same risk of overfitting.

Instead of RMSE, we will compute the **accuracy** of the logistic regression. Once again, closer to 1 is better.

```{r}
# Compute model accuracy.
fit |> 
  predict(new_data = soup_testing)|>
  bind_cols(soup_testing) |>
  accuracy(truth = top_selling, estimate = .pred_class)
```

--- 

Compute model accuracy with a custom threshold.

```{r}
threshold <- 0.6

# Compute model accuracy.
fit |> 
  predict(new_data = soup_testing, type = "prob") |>
  bind_cols(soup_testing) |>
  mutate(pred_y = if_else(.pred_1 >= threshold, 1, 0) |> factor()) |>
  accuracy(truth = top_selling, estimate = pred_y)
```

---

Accuracy can also be summarized using a **confusion matrix**.

```{r}
# Confusion matrix.
fit |> 
  predict(new_data = soup_testing) |>
  bind_cols(soup_testing) |>
  conf_mat(truth = top_selling, estimate = .pred_class)
```

## Predict | Counterfactual Predictions

Once we have a best-fitting model, we want to predict **counterfactuals**. A counterfactual is a prediction of what will happen if we intervene in the process in a certain way.

```{r}
# Column names, type, and preprocessing needs to match the recipe.
scenarios <- tibble(
  # generate spend on price decreases using 4 values from 0 to 10,000
  # repeat those spend values 4 times, one repetition for each trade area
  Any_Price_Decr_Spend = seq(from = 0, to = 10000, by = 10000 / 3) |> rep(4),
  # generate the trade areas, repeat 4 times for each value of spend on price decreases
  Retailer_Trade_Areas = unique(soup_data$Retailer_Trade_Areas) |> rep(4) |> sort(),
  top_selling = factor(1)
)
```

---

Remember to remove the outcome from `new_data` before using it in `predict()`.

```{r}
# Apply the recipe to the training data.
scenarios <- soup_recipe |>
  bake(scenarios) |> 
  select(-top_selling)

scenarios
```

---

Instead of predicting binary outcomes, we can also predict the probability of success.

```{r}
# Predict success (no prediction intervals).
predict(fit, new_data = scenarios, type = "prob") |> 
  bind_cols(scenarios) |> 
  mutate(Any_Price_Decr_Spend = exp(Any_Price_Decr_Spend)) |> 
  arrange(desc(.pred_1))
```

## Wrapping Up

*Summary*

- One last review of the inferential modeling workflow.
- Discussed the differences between regression and classification.
- Extended regression to a binary discrete outcome with logistic regression.

*Next Time*

- Project week.

## Live Coding

Once again, `iFood` has requested our help. They want statistically backed insights into which types of customers have been most likely to respond to their previous marketing campaign(s). Let's perform the appropriate pre-processing and modeling steps to answer their question.

## Exercise 12

Return one last time to `soup_data` and the models from the previous exercises 9, 10, 11.

1. Create a discrete outcome variable `top_selling` based on `Sales > 375000` using `if_else()` and `factor()`.
2. Preprocess the data using `initial_time_split()` with just 75% of the data in training and fit the same models again using the preprocessed training data, now as logistic regressions with the new `top_selling` variable as the outcome variable.
3. Compute the accuracy using the preprocessed testing data. Is the same model as last time the best-fitting model again?
4. Compute a confusion matrix for the best-fitting model and accurately interpret it.
5. Render the Quarto document into Word and upload to Canvas.
